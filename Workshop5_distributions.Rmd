---
title: "Distributions"
subtitle: "Workshop on Statistics for Linguistics"
author: "Sky Onosson, University of Manitoba"
output: 
  html_document:
    number_sections: true
---

<big>

```{r, include=FALSE, message=F, warning=F}
library("knitr")
opts_chunk$set(echo = TRUE)
```

---

## Agenda {-}

* Exploring distributions

---

## Load required packages and LIPP data {-}

```{r, message=F, warning=F}
if(!require(tidyverse)){install.packages("tidyverse")}
library(tidyverse)

if(!require(viridis)){install.packages("viridis")}
library(viridis)

LIPP <- read_csv("LIPP.csv")
```

# Exploring distributions

## Normal distributions

Let's look at the F2 quickplot from the previous workshop page again.

```{r}
qplot(LIPP$F2) +
  theme_light()
```

The shape we see here resembles a common pattern called the *normal distribution*, or sometimes a *bell curve*. The definition of a normal distribution is mathematically defined, and we won't go into it here. In most usages, it refers to a set of data which is uniformly distributed across a range such that the median and mean are identical. An important aspect of normal distributions is the *Central Limit Theorem* (CLT) which notes that as more data is added, most distributions become more normal.

We can generate a random normal distribution to see how this works; I'm using a formula which centers the distribution at 0, with a Standard Deviation of 100. We'll start with a distribution having only 10 observations, and increase this by powers of 10 until we get to 100,000 observations.

> **Note:** The code below includes a random 'seed' setting which ensures that the selection made is identical even when run at different times on different machines. If you change the seed to a different value, you will get different results, but the progression across the different observation magnitudes should be similar -- although it is possible to have a reversal where adding *more* observations causes the mean and median to drift (slightly) *further away* from each other.

```{r}
MyHistogram <- function(x) qplot(x, xlab = paste("n =", length(x), "; mean =", round(mean(x),3), "; median =", round(median(x),3), "; difference =", abs(round(mean(x),3)-round(median(x),3)))) + theme_light()
```

```{r RandHists, message=FALSE, warning=FALSE}
set.seed(11)
MyHistogram(rnorm(10,0,100))
MyHistogram(rnorm(100,0,100))
MyHistogram(rnorm(1000,0,100))
MyHistogram(rnorm(10000,0,100))
MyHistogram(rnorm(100000,0,100))
```

As you can see, at 10 observations there is very little evidence of any kind of structure, but even at 100 observations we can see something of a bell-like curve emerging as the mean and median approach each other. At 1,000 observations the difference between these measures is miniscule, and the distribution is clearly approaching the normal curve -- beyond this, at 10,000 or 100,000 observations, there are diminishing returns to be had as the difference between the mean and median approaches zero.

How "normal" is our F2 data that we looked at earlier? Let's review the frequency plot again.

``` {r}
ggplot(data = LIPP, aes(x = F2)) +
  geom_freqpoly(binwidth = 50) + 
  theme_light()
```

While the overall shape of this distribution is "bell-like" it is notably shifted or "skewed" towards one side, the lower end or left edge of the scale. Another useful visual assessment tool is the Q-Q "quantile-to-quantile" plot. The code below plots the Q-Q distribution of the data and then adds a diagonal line representing a normally-distributed dataset having the same mean and S.D. as the sample data.

```{r QQ1}
qqnorm(LIPP$F2)
qqline(LIPP$F2)
```

The closer a set of observations adhere to the diagonal line in a Q-Q plot, the more normal their distribution. Once again, we see that at the lower end of the scale the distribution skews away from the normal line *to some extent*. But what does this mean precisely?

To refine our visual observation that the distribution is mostly normal with some skewing, we can run a formal test. The Shapiro-Wilk normality test is useful for examining continuous distributions, such as vowel formant frequencies. This test as implemented in R does not function with datasets larger than n=5000. Since the number of LIPP F2 observations is greater than 16,000, we can simply restrict this to the first 5000 in running this test.

```{r}
length(LIPP$F2)
shapiro.test(LIPP$F2[1:5000])
```

The null hypothesis here is that the examined distribution is normal, so the low *p* value here indicates that we must *reject the null hypothesis* and conclude that the distribution is *not* normal (or, at least not perfectly normal). However, we also need to pay attention to the *W* score, which indicates the effect size. In this case, it indicates the degree of deviation from normality, with 1.0 indicating a perfectly normal distribution. The indicated value of 0.99658 is very, very close to this. Combining our observations of the frequency plot, Q-Q plot, and the Shapiro-Wilk test, we should conclude that the distribution is normal, but skewed slightly.

> See this page for a discussion of how the Shapiro-Wilk test performs over many random datasets, and in particular how to evaluate the *W*-score: https://emilkirkegaard.dk/en/?p=4452

Why test for normality? If a distribution is normal, this indicates the applicability of a range of parametric tests. However, for non-normal distributions, statisticians advise either transforming the data somehow, e.g. by taking the log or square root, or use one of a several non-parametric tests. In general parametric tests are more sensitive, especially for smaller datasets, so it is important to know how the data is distributed and take appropriate measures. However, as we'll see below, it often turns out that this is unnecessary when very large datasets are involved.

## Testing normal distributions across groups: *t*-tests, ANOVAs and related tests

We already looked at one parametric test: the *t*-test. Although we jumped in with it before we determined whether the data was normal, we have now determined that F2 is indeed normally distributed, so it was an appropriate test. The formula we used earlier evaluated the distributions of F2 split by speaker sex/gender, making this a *two-sample t*-test. To recall, that formula (and the result) was as follows.

```{r}
t.test(x = LIPP$F2[LIPP$sex == "male"], 
       y = LIPP$F2[LIPP$sex == "female"])
```

It's also possible to run a *one-sample t*-test, which we can easily do by simply putting the entire, non-subsetted F2 distribution in as the test variable.

```{r}
t.test(x = LIPP$F2)
```

What does this result tell us? As the *alternative hypothesis* comment notes, this test determines that the true mean is not equal to zero. A mean of zero is the default setting for a one-sample *t*-test, but we can substitute any number we like -- let's try with 1715 Hz, which is *very* close to the true mean.

```{r}
t.test(x = LIPP$F2, mu = 1715)
```

As we can see, the result here is still significant even in distinguishing between a mean and predicted value which are so close together, indicating how powerful the *t*-test can be. However, we should note carefully the difference between this and the earlier version where the mean was compared to a predicted value of zero: while both *p* values are quite small, indicative of significance (i.e. reject the null hypothesis), the *t* values are drastically different, indicating the relative effect size in each case. The first comparison to mean = zero has a *t* value of 1571.6, while the second comparison to mean = 1715 has a *t* value of only 6.4. So, the second test's output do tell us that the mean is definitely not 1715, but they also suggest that it's not very far away from that value, whereas the much higher *t* value of the first test indicates that the mean is indeed quite far from zero.

At any rate, when it comes to linguistic data, we are often looking at comparing across two groups, so the two-sample *t*-test is probably the more common implementation. Because the variable of sex/gender only has two levels in our data, male vs. female, the earlier 2-sample *t*-test covered everything we wanted to look at to evaluate the distributions relative to each other. Let's next explore the use of the *t*-test a bit more using some of the other variables in the data file. We can get a quick view of the whole data table by simply entering its name in R.

``` {r}
LIPP
```

This quick view gives us the first 10 rows (i.e. observations) and 8 columns (i.e. variables) -- we can also open the object from the Environment tab to view the entire data table for a more full view, but this is sufficient for now. Notice that we have both ``language`` and ``english_status`` variables. The latter refers to whether English (the language of the interviews) is the speaker's first or second language. We can see how this variable breaks down pretty simply using the `table` function.

``` {r}
table(LIPP$english_status)
```

We have pretty similar numbers of observations for each status. We can use the same *t*-test formula we performed earlier for speaker sex/gender, and see if F2 differs significantly according to language status by making its two levels the x and y variables.

``` {r}
t.test(x = LIPP$F2[LIPP$english_status == "L1"], 
       y = LIPP$F2[LIPP$english_status == "L2"])
```

Recall that a *t* value of +/- 1.96 is usually considered significant, and the result here is much smaller than this, so we can't conclude that these two groups have different distributions. We can also see that the mean values are nearly identical, so this result makes sense.

In thinking about this some more, however, we still might want to examine the effect of first language status further -- what if we compared speakers of different languages to each other? Let's see the breakdown of the `language` variable.

```{r}
table(LIPP$language)
```

English predominates, but we have at least 20,000 observations in each of the other three languages. Let's compare the two largest samples: English vs. Tagalog.

``` {r}
t.test(x = LIPP$F2[LIPP$language == "English"], 
       y = LIPP$F2[LIPP$language == "Tagalog"])
```

This time the *t* value is a little larger, though still small, and the means are a tiny bit further apart. How about English vs. the other two languages, Ilocano and Kapampangan?

``` {r}
t.test(x = LIPP$F2[LIPP$language == "English"], 
       y = LIPP$F2[LIPP$language == "Ilocano"])
```

``` {r}
t.test(x = LIPP$F2[LIPP$language == "English"], 
       y = LIPP$F2[LIPP$language == "Kapampangan"])
```

Now we get much larger *t* values, and both tests turn out to be significant, with mean F2 values that are more than 100 Hz apart, though in different directions: Ilocano speakers have a higher mean F2 indicating more fronted vowel production overall, while Kapampangan speakers have a lower F2, so more retracted vowel production overall.

> **Note:** We probably shouldn't conclude too much from these comparisons, as we are aggregating *all* of the vowels together here, so it's not very clear what these F2 differences mean. But we are just exploring the data here, and not trying to reach any large conclusions.

We started with a comparison of F2 across the two language statuses in the data, and got a non-significant result. But, when we dug down a little further using the 4-level `language` variable, we discovered that there were some hidden differences which were significant. Furthermore, we haven't even explored this completely, as we could compare each language against each other language, rather than only looking at English vs. the others one by one. That would involve three additional *t*-tests. And if our variable had more than four levels (such as `vowel`), we would need even more tests. There must be a simpler method!

R has a built-in test for doing pairwise comparisons, i.e. exhaustively testing one pair at a time from all groups being considered. ``pairwise.t.test()`` produces a table of *p* values for each comparison, however it does not provide any other analysis.

```{r}
pairwise.t.test(LIPP$F2, LIPP$language)
```

An alternative to this is the Analysis of Variance or ANOVA, which was designed to extend the power of the *t*-test beyond two groups or categories. The formula for an ANOVA is: dependent.variable ~ independent.variable.

```{r}
LIPPanova <- aov(F2 ~ language, data = LIPP)
summary(LIPPanova)
```

The ANOVA tests the null hypothesis that the dependent and independent variable are not correlated. Here, the very low *p* value indicates that there is good reason to reject this and conclude that there is a correlation -- but the ANOVA doesn't provide more detail than this on its own. However, it can be combined with the Tukey HSD ("honest significant difference") test to produce a more thorough analysis which includes pairwise comparisons across each group level. 

```{r}
TukeyHSD(LIPPanova)
```

Note that in each case, the pairwise test results indicate that only the Tagalog-English comparison produces a result which lacks statistical significance, although the exact *p* value differs in each case! This should be an indication that the *p* value does not always tell the whole story.

One other thing to consider is the level of variance across the groups which the data is split across when doing an ANOVA. If the variance is non-equal, then the test may not be appropriate. Plotting residuals is one way to do this, and can be called with the ``plot()`` function after conducting an ANOVA -- the residuals are the first plot.

```{r}
LIPPanova <- aov(F2 ~ language, data = LIPP)
plot(LIPPanova,1)
```

Interpreting this plot is difficult. If one group has residuals much further away from the x-axis than the other groups, this suggests unequal variances. Outliers are indicated with a reference number, but it isn't clear how many indicate non-equal variance, or how far they should be.

*Levene's test for homogeneity of variance* and the *Kruskal-Wallis rank sum test* are some other means of testing variance. The null hypothesis for both of these is that the variances present in each group's distribution are *not* significantly different -- so, a significant result here indicates that the variances *are* significantly different from each other.

```{r}
leveneTest(F2 ~ language, data = LIPP)
kruskal.test(F2 ~ language, data = LIPP)
```

The *p* values in each case are very low, indicating that the groups (i.e. speakers of different first languages) have significantly differ variances within their respective distributions. This suggests that our earlier ANOVA was perhaps not an appropriate test. Is there an alternative? In fact, yes -- Welch's one-way ANOVA doesn't assume equal variances.

```{r}
oneway.test(F2 ~ language, data = LIPP)
```

And furthermore, the pairwise *t*-test can be implemented for non-equal-variance comparisons by not pooling the standard deviations across groups.

```{r}
pairwise.t.test(LIPP$F2, LIPP$language, pool.sd = FALSE)
```
It turns out that we get virtually the same results with these two tests as we did earlier. What does this indicate? The Central Limit Theorem (CLT) tells us that repeated random sampling will produce means which fall into a normal distribution, even when the originating data does not contain a precisely normal distribution. The practical impact of this is that, for very large datasets, it's usually sufficient to use parametric tests (e.g. *t*-tests, ANOVAs, etc.) even when the data is not normally distributed, and even when you *know* that it isn't. The only real issue with the CLT is that it is not known precisely what constitutes a "large enough" dataset. For a dataset such as we are using here, with > 160,000 observations. If your dataset is much smaller and you're unsure whether to use a parametric or non-parametric test, it's probably advisable to use a non-parametric test.

Sonderegger et al. actually advise avoiding *t*-tests entirely and using the non-parametric Wilcox test.

```{r}
wilcox.test(LIPP$F2[LIPP$sex == "male"],
            LIPP$F2[LIPP$sex == "female"])
```

They note that for genuinely normal distributions, *t*-tests will produce more accurate *p* values, but that this is usually a small difference, and the Wilcox test is better as a general test. I don't have enough experience evaluating the difference between the two tests to make a strong claim one way or the other, but it is certain that the *t*-test is one of the most familiar statistical tests in wide usage, so it has a strong level of familiarity.

## Standard deviation, error and confidence intervals

One of the basic statistical measures is the *standard deviation*, often abbreviated S.D.

```{r}
sd(LIPP$F2)
```

S.D. is a measure of variance in the sample data being examined -- a low S.D. indicates that most values are close to the mean, while larger values indicate that they are more spread out. While S.D. is a measure of the sample which we have, *standard error (of the mean)* (S.E.)  tells us how likely it is that our sample -- or rather, its mean -- is a good estimate of the mean of larger poplulation which the sample represents, and which we can never accurately measure. There is no default function to calculate S.E. in R, but the formula is fairly straightforward, and can be calculated in two ways. 

```{r}
se1 <- function(x) sqrt(var(x)/length(x))
se2 <- function(x) sd(x)/sqrt(length(x))
se1(LIPP$F2)
se2(LIPP$F2)
```

The difference between S.D. and S.E. is that the former tells us about the distribution of variation within the sampled data, and S.E. tells us about how well our sample is representative of the larger population, and allows us to compute Confidence Intervals (CIs) which tell us where the true mean likely lies. If the S.E. is relatively small as compared to the S.D., then the confidence in our calculated mean falling close to the real mean is extremely high. CIs can be calculated for any percentage of the data range, but are typically calculated at 95% -- that is, the mean is estimated to fall within the CI range 95% of the time, which is generally accepted as the default for statistical signficance (cf. that 0.05 is the most commonly-used *p* value in many social sciences).

CIs at 95% are equivalent to 1.96 times the S.E. +/- the mean. We can calculate all of these individually, or the `ci()` function in the `gmodels` package offers this functionality. Both give identical results.

```{r}
# Mean of F2
mean(LIPP$F2)
# Standard Error of F2
se1(LIPP$F2)
# Lower 95% confidence interval of F2
mean(LIPP$F2)-1.96*se1(LIPP$F2)
# Upper 95% confidence interval of F2
mean(LIPP$F2)+1.96*se1(LIPP$F2)
library(gmodels)
ci(LIPP$F2)
```

</big>