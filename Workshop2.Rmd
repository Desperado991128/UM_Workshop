---
title: "Part Two: Distributions"
subtitle: "Workshop on Statistics for Linguistics"
author: "Sky Onosson, University of Manitoba"
output: 
  html_document:
    number_sections: true
---

<big>

```{r, include=FALSE, message=F, warning=F}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4, fig.path='Figs/')
```

# Preliminaries

This section is simply here to load the necessary libraries and the main LIPP data file.

```{r, message=F, warning=F}
library(dplyr)
library(readr)
library(ggplot2)
library(ggpubr)
library(viridisLite)
library(car)
LIPP <- read_csv("LIPP.csv")
```

# Exploring distributions

## Normal distributions

Let's look at the F2 quickplot from the previous workshop page again.

```{r}
qplot(LIPP$F2)
```

The shape we see here resembles a common pattern called the *normal distribution*, or sometimes a *bell curve*. The definition of a normal distribution is mathematically defined, and we won't go into it here. In most usages, it refers to a set of data which is uniformly distributed across a range such that the median and mean are identical. An important aspect of normal distributions is the *Central Limit Theorem* (CLT) which notes that as more data is added, most distributions become more normal.

We can generate a random normal distribution to see how this works; I'm using a formula which centers the distribution at 0, with a Standard Deviation of 100. We'll start with a distribution having only 10 observations, and increase this by powers of 10 until we get to 100,000 observations.

> **Note:** The code below includes a random 'seed' setting which ensures that the selection made is identical even when run at different times on different machines. If you change the seed to a different value, you will get different results, but the progression across the different observation magnitudes should be similar -- although it is possible to have a reversal where adding *more* observations causes the mean and median to drift (slightly) *further away* from each other.

```{r}
MyHistogram <- function(x) qplot(x, xlab = paste("n =", length(x), "; mean =", round(mean(x),3), "; median =", round(median(x),3), "; difference =", abs(round(mean(x),3)-round(median(x),3))))
```

```{r RandHists, message=FALSE, warning=FALSE}
set.seed(11)
MyHistogram(rnorm(10,0,100))
MyHistogram(rnorm(100,0,100))
MyHistogram(rnorm(1000,0,100))
MyHistogram(rnorm(10000,0,100))
MyHistogram(rnorm(100000,0,100))
```

As you can see, at 10 observations there is very little evidence of any kind of structure, but even at 100 observations we can see something of a bell-like curve emerging as the mean and median approach each other. At 1,000 observations the difference between these measures is miniscule, and the distribution is clearly approaching the normal curve -- beyond this, at 10,000 or 100,000 observations, there are diminishing returns to be had as the difference between the mean and median approaches zero.

How "normal" is our F2 data that we looked at earlier? Let's review the frequency plot again.

``` {r}
ggplot(data = LIPP, aes(x = F2)) +
  geom_freqpoly(binwidth = 50)
```

While the overall shape of this distribution is "bell-like" it is notably shifted or "skewed" towards one side, the lower end or left edge of the scale. Another useful visual assessment tool is the Q-Q "quantile-to-quantile" plot, which we can access with the ``ggpplot()`` function from the ``ggpubr`` package.

```{r QQ}
ggqqplot(LIPP$F2)
```

The closer a set of observations adhere to the diagonal line in a Q-Q plot, the more normal their distribution. Once again, we see that at the lower end of the scale the distribution skews away from the normal line *to some extent*. But what does this mean precisely?

To refine our visual observation that the distribution is mostly normal with some skewing, we can run a formal test. The Shapiro-Wilk normality test is useful for examining continuous distributions, such as vowel formant frequencies. This test as implemented in R does not function with datasets larger than n=5000. Since the number of LIPP F2 observations is greater than 16,000, we can simply restrict this to the first 5000 in running this test.

```{r}
length(LIPP$F2)
shapiro.test(LIPP$F2[1:5000])
```

The null hypothesis here is that the examined distribution is normal, so the low *p*-value here indicates that we must *reject the null hypothesis* and conclude that the distribution is *not* normal (or, at least not perfectly normal). However, we also need to pay attention to the *W* score, which indicates the effect size. In this case, it indicates the degree of deviation from normality, with 1.0 indicating a perfectly normal distribution. The indicated value of 0.99658 is very, very close to this. Combining our observations of the frequency plot, Q-Q plot, and the Shapiro-Wilk test, we should conclude that the distribution is normal, but skewed slightly.

> See this page for a discussion of how the Shapiro-Wilk test performs over many random datasets, and in particular how to evaluate the *W*-score: https://emilkirkegaard.dk/en/?p=4452

Why even test for normality? If a distribution is normal, this indicates the applicability of a range of parametric tests. However, for non-normal distributions we must either transform the data somehow, e.g. by taking the log or square root, or use one of a different set of non-parametric tests. In general parametric tests are more sensitive, espeically for smaller datasets, so it is important to know how the data is distributed and take appropriate measures.

## Testing normal distributions across groups

We already looked at one parametric test: the *t*-test. Although we jumped in with it before we determined whether the data was normal, we have now determined that F2 is indeed normally distributed, so it was an appropriate test. The formula we used earlier evaluated the distributions of F2 split by speaker sex/gender, making this a *two-sample t*-test. To recall, that formula (and the result) was as follows.

```{r}
t.test(x = LIPP$F2[LIPP$sex == "male"], 
       y = LIPP$F2[LIPP$sex == "female"])
```

It's also possible to run a *one-sample t*-test, which we can easily do by simply putting the entire, non-subsetted F2 distribution in as the test variable.

```{r}
t.test(x = LIPP$F2)
```

What does this result tell us? As the *alternative hypothesis* comment notes, this test determines that the true mean is not equal to zero. A mean of zero is the default setting for a one-sample *t*-test, but we can substitute any number we like -- let's try with 1715 Hz, which is *very* close to the true mean.

```{r}
t.test(x = LIPP$F2, mu = 1715)
```

As we can see, the result here is still significant even in distinguishing between a mean and predicted value which are so close together, indicating how powerful the *t*-test can be. However, we should note carefully the difference between this and the earlier version where the mean was compared to a predicted value of zero: while both *p*-values are quite small, indicative of significance (i.e. reject the null hypothesis), the *t*-values are drastically different, indicating the relative effect size in each case. The first comparison to mean = zero has a *t*-value of 1571.6, while the second comparison to mean = 1715 has a *t*-value of only 6.4. So, the second test's output do tell us that the mean is definitely not 1715, but they also suggest that it's not very far away from that value, whereas the much higher *t*-value of the first test indicates that the mean is indeed quite far from zero.

At any rate, when it comes to linguistic data, we are often looking at comparing across two groups, so the two-sample *t*-test is probably the more common implementation. Because the variable of sex/gender only has two levels in our data, male vs. female, the earlier 2-sample *t*-test covered everything we wanted to look at to evaluate the distributions relative to each other. Let's next explore the use of the *t*-test a bit more using some of the other variables in the data file. We can get a quick view of the whole data table by simply entering its name in R.

``` {r}
LIPP
```

This quick view gives us the first 10 rows (i.e. observations) and 6 columns (i.e. variables) -- we can also open the object from the Environment tab to view the entire data table for a more full view, but this is sufficient for now. Notice that we have both ``language`` and ``lang_status`` variables. The latter refers to whether English (the language of the interviews) is the speaker's first or second language. We can see how this variable breaks down pretty simply using the `table` function.

``` {r}
table(LIPP$lang_status)
```

We have pretty similar numbers of observations for each status. We can use the same *t*-test formula we performed earlier for speaker sex/gender, and see if F2 differs significantly according to language status by making its two levels the x and y variables.

``` {r}
t.test(x = LIPP$F2[LIPP$lang_status == "L1"], 
       y = LIPP$F2[LIPP$lang_status == "L2"])
```

Recall that a *t* value of +/- 1.96 is usually considered significant, and the result here is much smaller than this, so we can't conclude that these two groups have different distributions. We can also see that the mean values are nearly identical, so this result makes sense.

In thinking about this some more, however, we still might want to examine the effect of first language status further -- what if we compared speakers of different languages to each other? Let's see the breakdown of the `language` variable.

```{r}
table(LIPP$language)
```

English predominates, but we have at least 20,000 observations in each of the other three languages. Let's compare the two largest samples: English vs. Tagalog.

``` {r}
t.test(x = LIPP$F2[LIPP$language == "English"], 
       y = LIPP$F2[LIPP$language == "Tagalog"])
```

This time the *t* value is a little larger, though still small, and the means are a tiny bit further apart. How about English vs. the other two languages, Ilocano and Kapampangan?

``` {r}
t.test(x = LIPP$F2[LIPP$language == "English"], 
       y = LIPP$F2[LIPP$language == "Ilocano"])
```

``` {r}
t.test(x = LIPP$F2[LIPP$language == "English"], 
       y = LIPP$F2[LIPP$language == "Kapampangan"])
```

Now we get much larger *t* values, and both tests turn out to be significant, with mean F2 values that are more than 100 Hz apart, though in different directions: Ilocano speakers have a higher mean F2 indicating more fronted vowel production overall, while Kapampangan speakers have a lower F2, so more retracted vowel production overall.

> **Note:** We probably shouldn't conclude too much from these comparisons, as we are aggregating *all* of the vowels together here, so it's not very clear what these F2 differences mean. But we are just exploring the data here, and not trying to reach any large conclusions.

We started with a comparison of F2 across the two language statuses in the data, and got a non-significant result. But, when we dug down a little further using the 4-level `language` variable, we discovered that there were some hidden differences which were significant. Furthermore, we haven't even explored this completely, as we could compare each language against each other language, rather than only looking at English vs. the others one by one. That would involve three additional *t*-tests. And if our variable had more than four levels (such as `vowel`), we would need even more tests. There must be a simpler method!

R has a built-in test for doing pairwise comparisons, i.e. exhaustively testing one pair at a time from all groups being considered. ``pairwise.t.test()`` produces a table of *p*-values for each comparison, however it does not provide any other analysis.

```{r}
pairwise.t.test(LIPP$F2, LIPP$language)
```

An alternative to this is the Analysis of Variance or ANOVA, which was designed to extend the power of the *t*-test beyond two groups or categories. This can be combined with the Tukey HSD ("honest significant difference") test to produce a more thorough analysis. The formula for an ANOVA is: dependent.variable ~ independent.variable.

```{r}
LIPPanova <- aov(F2 ~ language, data = LIPP)
summary(LIPPanova)
TukeyHSD(LIPPanova)
```

Note that in each case, the pairwise test results indicate that only the Tagalog-English comparison produces a result which lacks statistical significance, although the exact *p* value differs in each case! This should be an indication that the *p* value does not always tell the whole story.

One other thing to consider is the level of variance across the groups which the data is split across when doing an ANOVA. If the variance is non-equal, then the test may not be appropriate. Plotting residuals is one way to do this, and can be called with the ``plot()`` function after conducting an ANOVA -- the residuals are the first plot.

```{r}
LIPPanova <- aov(F2 ~ language, data = LIPP)
plot(LIPPanova,1)
```

Interpreting this plot is difficult. If one group has residuals much further away from the x-axis than the other groups, this suggests unequal variances. Outliers are indicated with a reference number, but it isn't clear how many indicate non-equal variance, or how far they should be.

*Levene's test for homogeneity of variance* and the *Kruskal-Wallis rank sum test* are some other means of testing variance. The null hypothesis for both of these is that the variances present in each group's distribution are *not* significantly different -- so, a significant result here indicates that the variances *are* significantly different from each other.

```{r}
leveneTest(F2 ~ language, data = LIPP)
kruskal.test(F2 ~ language, data = LIPP)
```

The *p* values in each case are very low, indicating that the groups (i.e. speakers of different first languages) have significantly differ variances within their respective distributions. This suggests that our earlier ANOVA was perhaps not an appropriate test. Is there an alternative? In fact, yes -- Welch's one-way ANOVA doesn't assume equal variances.

```{r}
oneway.test(F2 ~ language, data = LIPP)
```

And furthermore, the pairwise *t*-test can be implemented for non-equal-variance comparisons by not pooling the standard deviations across groups.

```{r}
pairwise.t.test(LIPP$F2, LIPP$language, pool.sd = FALSE)
```

It turns out that we get virtually the same results with these two tests as we did earlier. What does this indicate? To be honest, I am not sure, but would suggest preceding with caution. Run ANOVAs when dealing with multiple-level categories (more than two), but check for equal variance, and then run tests such as the one-way test or the pairwise t-test which don't depend on variance assumptions. If your results coincide with the initial ANOVA, then you haven't lost anything and can be more confident with your results.









</big>